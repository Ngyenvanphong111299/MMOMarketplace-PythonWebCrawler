[
{"title": "Introducing IndQA", "link": "https://openai.com/index/introducing-indqa/", "description": "A new benchmark for evaluating AI systems on Indian culture and languages.", "content": "November 3, 2025\n\nResearch\n\nRelease\n\nIntroducing IndQA\n\nA new benchmark for evaluating AI systems on Indian culture and languages.\n\nListen to article\n\nOur mission is to make AGI benefit all of humanity. If AI is going to be useful for everyone, it needs to work well across languages and cultures. About 80 percent of people worldwide do not speak English as their primary language, yet most existing benchmarks that measure non-English language capabilities fall short.\n\nExisting multilingual benchmarks like\n\n(opens in a new window)\n\nare now saturated—top models cluster near high scores—which make them less useful for measuring real progress. In addition, current benchmarks mostly focus on translation or multiple-choice tasks. They don’t adequately capture what really matters for evaluating an AI system’s language capabilities—understanding context, culture, history, and the things that matter to people where they live.\n\nThat’s why we built\n\n, a new benchmark designed to evaluate how well AI models understand and reason about questions that matter in Indian languages, across a wide range of cultural domains. While our aim is to create similar benchmarks for other languages and regions, India is an obvious starting point. India has about a billion people who don’t use English as their primary language, 22 official languages (including at least seven with over 50 million speakers), and is ChatGPT’s second largest market.\n\nThis work is part of our ongoing commitment to improve our products and tools for Indian users, and to make our technology more accessible throughout the country.\n\nHow it works\n\nIndQA evaluates knowledge and reasoning about Indian culture and everyday life in Indian languages. It spans 2,278 questions across 12 languages and 10 cultural domains, created in partnership with 261 domain experts from across India. Unlike existing benchmarks like MMMLU and MGSM, it is designed to probe culturally nuanced, reasoning-heavy tasks that existing evaluations struggle to capture.\n\nIndQA covers a broad range of culturally relevant topics, such as\n\nArchitecture & Design, Arts & Culture, Everyday Life, Food & Cuisine, History, Law & Ethics, Literature & Linguistics, Media & Entertainment, Religion & Spirituality,\n\nSports & Recreation\n\n—with items written natively in\n\nBengali, English, Hindi, Hinglish, Kannada, Marathi, Odia, Telugu, Gujarati, Malayalam, Punjabi,\n\nNote: We specifically added Hinglish given the prevalence of code-switching in conversations.\n\nEach datapoint includes a\n\nculturally grounded prompt\n\nin an Indian language, an\n\nEnglish translation\n\nfor auditability,\n\nrubric criteria\n\nfor grading, and an\n\nideal answer\n\nthat reflects expert expectations.\n\nIndQA uses a rubric-based approach. Each response is graded against criteria written by domain experts for that specific question. The criteria spell out what an ideal answer should include or avoid, and each one is given a weighted point value based on its importance. A model-based grader checks whether each criterion is met. The final score is the sum of the points for criteria satisfied out of the total possible.\n\nHow we built IndQA\n\nExpert‑authored questions.\n\nWe worked with partners to find experts in India across 10 different domains. They drafted difficult, reasoning‑focused prompts tied to their regions and specialties. These experts are native‑level speakers of the relevant language (and English) and bring deep subject expertise.\n\nAdversarial filtering:\n\nEach question was tested against OpenAI’s strongest models at the time of their creation: GPT‑4o, OpenAI o3, GPT‑4.5, and (partially, post public launch) GPT‑5. We kept only those questions where a majority of these models failed to produce acceptable answers, preserving headroom for progress\n\nDetailed Criteria.\n\nAlong with every question, domain experts provided criteria used to grade the model response, similar to an exam rubric for an essay question. These criteria are used to grade responses from candidate models.\n\nIdeal answers + review.\n\nExperts added ideal answers and English translations, followed by peer review and iterative fixes until sign‑off.\n\nExample questions\n\nBengali\n\nGujarati\n\nHinglish\n\nKannada\n\nMalayalam\n\nMarathi\n\nPunjabi\n\nTelugu\n\nLanguage: Bengali\n\nDomain: Literature and linguistics\n\nPrompt\n\n‘দণ্ডক থেকে মরিচঝাঁপি’ উপন্যাসের লেখক নিম্নবর্ণের পুরুষ ও নারীদের দণ্ডকারন্যে পুনর্বাসন পরবর্তী জীবন কিভাবে দেখিয়েছেন? দণ্ডকারণ্যে পুনর্বাসন কি সরকারী উদাসীনতার ফল? পরিবর্তিত প্রাকৃতিক পরিবেশের সাথে উদ্বাস্তুরা কিভাবে মানিয়ে নিয়েছিল?\n\nEnglish Translation\n\nHow did the writer of Bengali novel ‘Dandak Theke Marichjhanpi’ depict the post-rehabilitation lives of lower caste men and women? Was the rehabilitation in Dandakaranya a result of governmental indifference? What was its relation with the new natural landscapes?\n\nDomain: Food and cuisine\n\nPrompt\n\nকোন পরিপ্রেক্ষিতে উনিশ শতকের শেষ দিক থেকে রান্নার বইগুলো বেরচ্ছিল ? প্রথম বাংলা রান্নার বইটির সাথে বিপ্রদাস মুখোপাধ্যায় রচিত বইটির পার্থক্য কোথায় ? বিপ্রদাসের উদ্যোগে প্রকাশিত পত্রিকাটি চলেছিল কতদিন ? বিপ্রদাস ও প্রজ্ঞা সুন্দরীর লেখা অনুসরণ করে দিঘাপতিয়া থেকে কোন বইটি বেরিয়েছিল ?\n\nEnglish Translation\n\nIn what context were cookbooks published from the end of the 19th century? What is the difference between the first Bengali cookbook and the book written by Bipradas Mukherjee? How long did the magazine published by Bipradas run? Which book was published by Dighapatiya following the writings of Bipradas and Pragya Sundari?\n\nImprovements over time\n\nWe use IndQA to evaluate how recent frontier models perform and chart progress over the last couple years. With IndQA we can see that OpenAI’s models have improved significantly over time on Indian languages (with\n\ncaveats\n\n), but still have substantial room for improvement. We look forward to improving performance and sharing results for future models.\n\nWe also stratify performance on IndQA by Language and Domain below, comparing GPT‑5 Thinking High to other frontier models.\n\nCaveats\n\nBecause questions are\n\nnot identical\n\nacross languages, IndQA is\n\na language leaderboard; cross‑language scores shouldn’t be interpreted as direct comparisons of language ability. Instead, we plan to use IndQA to measure\n\nimprovement over time\n\nwithin a model family or configuration.\n\nAdditionally, because questions were filtered to those GPT‑4o, OpenAI o3, GPT‑4.5, and (post public launch) GPT‑5 could not answer sufficiently, question selection is adversarial against these models. This potentially confounds the relative performance of GPT‑5, and could disadvantage all OpenAI models compared to non-OpenAI models.\n\nThe experts behind IndQA\n\nWe’re grateful to the\n\nIndian experts—journalists, linguists, scholars, artists, and industry practitioners—who authored and reviewed questions for IndQA. A few examples of the experts we worked with includes:\n\nA Nandi Award winning Telugu actor and screenwriter with over 750 films\n\nA Marathi journalist and editor at Tarun Bharat\n\nA scholar of Kannada linguistics and dictionary editor\n\nAn International Chess Grandmaster who coaches top-100 chess players\n\nA Tamil writer, poet, and cultural activist advocating for social justice, caste equity, and literary freedom\n\nAn award winning Punjabi music composer\n\nA Gujarati heritage curator and conservation specialist\n\nAn award winning Malayalam poet and performance artist\n\nA professor of history, specializing in Bengal's rich cultural heritage\n\nA professor of architecture, focusing on Odishan temples\n\nNext steps\n\nWe hope the release of IndQA will inform and inspire new benchmark creation from the research community. IndQA style questions are especially valuable in languages or cultural domains that are poorly covered by existing AI benchmarks. Creating similar benchmarks to IndQA can help AI research labs learn more about languages and domains models struggle with today, and provide a north star for improvements in the future.\n\nLanguage\n\nReasonings & Policy\n\nAuthor\n\nOpenAI\n\nKeep reading\n\nView all\n\nIntroducing gpt-oss-safeguard\n\nProduct\n\nOct 29, 2025\n\nTechnical Report: Performance and baseline evaluations of gpt-oss-safeguard-120b and gpt-oss-safeguard-20b\n\nSafety\n\nOct 29, 2025\n\nDefining and evaluating political bias in LLMs\n\nResearch\n\nOct 9, 2025", "content_length": 8265, "authors": null, "date": "2025-10-29T00:00", "tags": null, "images": ["https://images.ctfassets.net/kftzwdyauwt9/5RrMpGwZVvEFy6d02cXoOq/4891d9b5c952037c97d20041acd9675f/oai_IndQA_16.9.png?w=1600&h=900&fit=fill", "https://images.ctfassets.net/kftzwdyauwt9/5RrMpGwZVvEFy6d02cXoOq/4891d9b5c952037c97d20041acd9675f/oai_IndQA_16.9.png?w=3840&q=90&fm=webp", "https://images.ctfassets.net/kftzwdyauwt9/5hXCmxfwL3BwhXddew3YFc/95baaee978fdf6416beb65f27780a37b/oai_IndQA_eval_â__Â_Desktop__Light_.svg?w=3840&q=90", "https://images.ctfassets.net/kftzwdyauwt9/70DRKmUAlpyn5xyOoZ6Qc3/d56ae272a76f3975dca778837481627a/gpt-oss-safeguard_Art_Card_1.1.png?w=3840&q=90&fm=webp", "https://images.ctfassets.net/kftzwdyauwt9/2TVxGPt7HbxPU6WbowpCVU/eaee5b4f389a913bd86728a33231ede3/gpt-oss-safeguard_SystemCard_1.1.png?w=3840&q=90&fm=webp", "https://images.ctfassets.net/kftzwdyauwt9/7am3PUXxkb4XCEDkcpRcSq/440a39e07303b7aa9e38552d2e8e50d8/political-bias-in-llms-1.1.png?w=3840&q=90&fm=webp"]}
]