#!/bin/bash
# Script deploy helper - cÃ³ thá»ƒ cháº¡y thá»§ cÃ´ng trÃªn server
# Sá»­ dá»¥ng script nÃ y Ä‘á»ƒ test deploy trÆ°á»›c khi setup auto-deploy

set -e

# Configuration
REGISTRY="ghcr.io"
IMAGE_NAME="python-web-scraping"
REPO_NAME="OWNER/REPO"  # Thay báº±ng repo cá»§a báº¡n
GHCR_TOKEN="${GHCR_TOKEN}"  # GitHub PAT token
GHCR_USERNAME="${GHCR_USERNAME}"  # GitHub username
API_KEY="${API_KEY:-XzEcSl7aaW7wfeyxW74IGpGDBcM4noaO}"
DEPLOY_PORT="${DEPLOY_PORT:-8000}"
DATA_VOLUME_PATH="${DATA_VOLUME_PATH:-/app/data}"

echo "ğŸš€ Báº¯t Ä‘áº§u deploy..."

# Login vÃ o GitHub Container Registry
echo "${GHCR_TOKEN}" | docker login ${REGISTRY} -u ${GHCR_USERNAME} --password-stdin

# Pull image má»›i nháº¥t
echo "ğŸ“¥ Pulling image..."
docker pull ${REGISTRY}/${REPO_NAME}/${IMAGE_NAME}:latest

# Dá»«ng vÃ  xÃ³a container cÅ©
echo "ğŸ›‘ Stopping old container..."
docker stop python-web-scraping || true
docker rm python-web-scraping || true

# Cháº¡y container má»›i
echo "â–¶ï¸ Starting new container..."
docker run -d \
  --name python-web-scraping \
  --restart unless-stopped \
  -p ${DEPLOY_PORT}:8000 \
  -v ${DATA_VOLUME_PATH}:/app/mycrawler/data \
  -e API_KEY="${API_KEY}" \
  -e RATE_LIMIT_PER_MINUTE="${RATE_LIMIT_PER_MINUTE:-60}" \
  -e ALLOWED_ORIGINS="${ALLOWED_ORIGINS:-http://localhost:3000}" \
  -e ALLOWED_DOMAINS="${ALLOWED_DOMAINS:-openai.com,techcrunch.com,anthropic.com,adobe.com}" \
  ${REGISTRY}/${REPO_NAME}/${IMAGE_NAME}:latest

# Cleanup old images
echo "ğŸ§¹ Cleaning up old images..."
docker image prune -f

# Kiá»ƒm tra container Ä‘ang cháº¡y
echo "â³ Waiting for container to start..."
sleep 10

if docker ps | grep -q python-web-scraping; then
  echo "âœ… Container Ä‘ang cháº¡y!"
  
  # Health check
  echo "ğŸ¥ Running health check..."
  sleep 5
  if curl -f http://localhost:${DEPLOY_PORT}/ > /dev/null 2>&1; then
    echo "âœ… Health check passed!"
    echo "ğŸš€ Deploy thÃ nh cÃ´ng!"
    exit 0
  else
    echo "âŒ Health check failed!"
    exit 1
  fi
else
  echo "âŒ Container khÃ´ng cháº¡y!"
  docker logs python-web-scraping
  exit 1
fi

